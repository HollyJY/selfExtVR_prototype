stt:
  device: cpu             # cpu or cuda
  model_size: base.en     # tiny base small medium large
  vad: true

llm:
  device: cuda
  model_name: Llama-3-8B-Instruct
  precision: 4bit        # 4bit 8bit fp16

tts:
  device: cpu            # switch to cuda if needed
  voice: npc_barista_friendly
  sample_rate: 48000
  format: wav

http:
  stt_port: 7001
  llm_port: 7002
  tts_port: 7003

paths:
  data_root: /workspace/data
  sessions_dir: /workspace/data/sessions
  exports_dir: /workspace/data/exports
  logs_dir: /workspace/logs
  models_root: /workspace/models
  # ollama_models_dir: /workspace/models/llama   # Optional: Ollama models dir. Defaults to "{models_root}/llama" if unset.
  # hf_cache_dir: /workspace/models/hf-cache     # Optional: Hugging Face cache dir (HF_HOME/TRANSFORMERS_CACHE).

locks:
  enable_gpu_lock: true
  lock_path: /workspace/data/.gpu.lock
